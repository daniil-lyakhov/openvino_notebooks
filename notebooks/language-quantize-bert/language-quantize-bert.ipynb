{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Quantize NLP models with Post-Training Quantization ​in NNCF\n",
    "This tutorial demonstrates how to apply `INT8` quantization to the Natural Language Processing model known as [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)), using the [Post-Training Quantization API](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.html) (NNCF library). A fine-tuned [HuggingFace BERT](https://huggingface.co/transformers/model_doc/bert.html) [PyTorch](https://pytorch.org/) model, trained on the [Microsoft Research Paraphrase Corpus (MRPC)](https://www.microsoft.com/en-us/download/details.aspx?id=52398), will be used. The tutorial is designed to be extendable to custom models and datasets. It consists of the following steps:\n",
    "\n",
    "- Download and prepare the BERT model and MRPC dataset.\n",
    "- Define data loading and accuracy validation functionality.\n",
    "- Prepare the model for quantization.\n",
    "- Run optimization pipeline.\n",
    "- Load and test quantized model.\n",
    "- Compare the performance of the original, converted and quantized models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d89f8a6b",
   "metadata": {},
   "source": [
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Imports](#Imports)\n",
    "- [Settings](#Settings)\n",
    "- [Prepare the Model](#Prepare-the-Model)\n",
    "- [Prepare the Dataset](#Prepare-the-Dataset)\n",
    "- [Optimize model using NNCF Post-training Quantization API](#Optimize-model-using-NNCF-Post-training-Quantization-API)\n",
    "- [Load and Test OpenVINO Model](#Load-and-Test-OpenVINO-Model)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "- [Compare F1-score of FP32 and INT8 models](#Compare-F1-score-of-FP32-and-INT8-models)\n",
    "- [Compare Performance of the Original, Converted and Quantized Models](#Compare-Performance-of-the-Original,-Converted-and-Quantized-Models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694d9fc1-501c-4b86-a747-637e2aad64ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"nncf>=2.5.0\"\n",
    "%pip install -q torch transformers \"torch>=2.1\" datasets evaluate tqdm  --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"openvino>=2023.1.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771388d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from typing import Iterable\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import nncf\n",
    "from nncf.parameters import ModelType\n",
    "import openvino as ov\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9e66896-d439-4065-868a-65b44d31525a",
   "metadata": {},
   "source": [
    "## Settings\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284e9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data and model directories, source URL and the filename of the model.\n",
    "DATA_DIR = \"data\"\n",
    "MODEL_DIR = \"model\"\n",
    "MODEL_LINK = \"https://download.pytorch.org/tutorial/MRPC.zip\"\n",
    "FILE_NAME = MODEL_LINK.split(\"/\")[-1]\n",
    "PRETRAINED_MODEL_DIR = os.path.join(MODEL_DIR, \"MRPC\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44dc335d",
   "metadata": {
    "id": "YytHDzLE0uOJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Perform the following:\n",
    "\n",
    "- Download and unpack pre-trained BERT model for MRPC by PyTorch.\n",
    "- Convert the model to the OpenVINO Intermediate Representation (OpenVINO IR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9fc64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70bfb62bf534d49aad8d23d057ce6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model/MRPC.zip:   0%|          | 0.00/387M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_file(MODEL_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "with ZipFile(f\"{MODEL_DIR}/{FILE_NAME}\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(MODEL_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dca2fa0",
   "metadata": {
    "id": "ehX7F6KB0uPu"
   },
   "source": [
    "Convert the original PyTorch model to the OpenVINO Intermediate Representation.\n",
    "\n",
    "From OpenVINO 2023.0, we can directly convert a model from the PyTorch format to the OpenVINO IR format using model conversion API. Following PyTorch model formats are supported:\n",
    "\n",
    "- `torch.nn.Module`\n",
    "- `torch.jit.ScriptModule`\n",
    "- `torch.jit.ScriptFunction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2f6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "input_shape = ov.PartialShape([1, -1])\n",
    "ir_model_xml = Path(MODEL_DIR) / \"bert_mrpc.xml\"\n",
    "core = ov.Core()\n",
    "\n",
    "torch_model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "torch_model.eval\n",
    "\n",
    "input_info = [\n",
    "    (\"input_ids\", input_shape, np.int64),\n",
    "    (\"attention_mask\", input_shape, np.int64),\n",
    "    (\"token_type_ids\", input_shape, np.int64),\n",
    "]\n",
    "default_input = torch.ones(1, MAX_SEQ_LENGTH, dtype=torch.int64)\n",
    "inputs = {\n",
    "    \"input_ids\": default_input,\n",
    "    \"attention_mask\": default_input,\n",
    "    \"token_type_ids\": default_input,\n",
    "}\n",
    "\n",
    "# Convert the PyTorch model to OpenVINO IR FP32.\n",
    "if not ir_model_xml.exists():\n",
    "    model = ov.convert_model(torch_model, example_input=inputs, input=input_info)\n",
    "    ov.save_model(model, str(ir_model_xml))\n",
    "else:\n",
    "    model = core.read_model(ir_model_xml)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17f79b5f",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "We download the [General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) dataset for the MRPC task from HuggingFace datasets.\n",
    "Then, we tokenize the data with a pre-trained BERT tokenizer from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "632fb1fc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01c41b8385b417bbf04d13c9105c1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5d5c08967b4ffcbcae77223b82b0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaa138fb137419a901d0e8ace41fc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eefba147b9b445d87cfd1dd2faf908a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a423d2c1ba4b8ab95c8db4d5a5c861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48895554a5fd48eb9c0d644255ea4c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbfbd0bd0754d039a1e56d516a25486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7d5076257549e6bb06e796113c5d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2828a47f1da749a3b74a896fb476fcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_data_source():\n",
    "    raw_dataset = datasets.load_dataset(\"glue\", \"mrpc\", split=\"validation\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "\n",
    "    def _preprocess_fn(examples):\n",
    "        texts = (examples[\"sentence1\"], examples[\"sentence2\"])\n",
    "        result = tokenizer(*texts, padding=\"max_length\", max_length=MAX_SEQ_LENGTH, truncation=True)\n",
    "        result[\"labels\"] = examples[\"label\"]\n",
    "        return result\n",
    "\n",
    "    processed_dataset = raw_dataset.map(_preprocess_fn, batched=True, batch_size=1)\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "data_source = create_data_source()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f36896fd",
   "metadata": {},
   "source": [
    "## TORCH FX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f6fbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\n",
    "from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer\n",
    "from torch.ao.quantization.quantize_pt2e import convert_pt2e\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_pt2e\n",
    "from torch.export import Dim\n",
    "from itertools import islice\n",
    "\n",
    "constraint = Dim(\"ids_att_mask_const\", max=76)\n",
    "dynamic_shapes = {\"input_ids\": {1: constraint}, \"attention_mask\": {1: constraint},\n",
    "                  \"pixel_values\": {}}\n",
    "\n",
    "def quantize(model, calibration_dataset, subset_size: int = 300):\n",
    "    example_kwargs = next(iter(calibration_dataset))\n",
    "    with torch.no_grad():\n",
    "        # Static\n",
    "        exported_model = capture_pre_autograd_graph(model, args=(), kwargs=example_kwargs)\n",
    "        # Dynamic\n",
    "        #exported_model = capture_pre_autograd_graph(model, args=(), kwargs=example_kwargs,\n",
    "        #                                            dynamic_shapes=dynamic_shapes)\n",
    "        # torch 2.3.1\n",
    "        #exported_model = capture_pre_autograd_graph(model, args=(), kwargs=example_kwargs, constraints=[\n",
    "        #    dynamic_dim(example_kwargs[\"input_ids\"], 1) == dynamic_dim(example_kwargs[\"attention_mask\"], 1),\n",
    "        #    # Torch.fx said I need this  constraint as well 0-0\n",
    "        #    dynamic_dim(example_kwargs[\"input_ids\"], 1) < 77,\n",
    "        #])\n",
    "\n",
    "    open(\"c.py\", \"w\").write(exported_model.code)\n",
    "    quantizer = X86InductorQuantizer()\n",
    "    quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n",
    "\n",
    "    prepared_model = prepare_pt2e(exported_model, quantizer)\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for inp  in islice(tqdm(calibration_dataset), subset_size):\n",
    "        # Static\n",
    "        # prepared_model(**example_kwargs)\n",
    "        # Dynamic\n",
    "        prepared_model(**inp)\n",
    "    converted_model = convert_pt2e(prepared_model)\n",
    "    return converted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19413cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAMES = [key for key in inputs.keys()]\n",
    "\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    \"\"\"\n",
    "    Extract the model's input from the data item.\n",
    "    The data item here is the data item that is returned from the data source per iteration.\n",
    "    This function should be passed when the data item cannot be used as model's input.\n",
    "    \"\"\"\n",
    "    inputs = {name: np.asarray([data_item[name]], dtype=np.int64) for name in INPUT_NAMES}\n",
    "    return inputs\n",
    "\n",
    "\n",
    "calibration_dataset = nncf.Dataset(data_source, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8fef3ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unsupported",
     "evalue": "call_function ConstantVariable(int: 128) [] {}\n\nfrom user code:\n   File \"/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1564, in forward\n    outputs = self.bert(\n  File \"/home/dlyakhov/env/tmp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 961, in forward\n    input_shape = input_ids.size()\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m quantized_model \u001b[39m=\u001b[39m quantize(torch_model, calibration_dataset\u001b[39m.\u001b[39;49mget_inference_data())\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mquantize\u001b[0;34m(model, calibration_dataset, subset_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m example_kwargs \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(calibration_dataset))\n\u001b[1;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     17\u001b[0m     \u001b[39m# Static\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     exported_model \u001b[39m=\u001b[39m capture_pre_autograd_graph(model, args\u001b[39m=\u001b[39;49m(), kwargs\u001b[39m=\u001b[39;49mexample_kwargs)\n\u001b[1;32m     19\u001b[0m     \u001b[39m# Dynamic\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m#exported_model = capture_pre_autograd_graph(model, args=(), kwargs=example_kwargs,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m#                                            dynamic_shapes=dynamic_shapes)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39m#    dynamic_dim(example_kwargs[\"input_ids\"], 1) < 77,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m#])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mc.py\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mwrite(exported_model\u001b[39m.\u001b[39mcode)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_export/__init__.py:151\u001b[0m, in \u001b[0;36mcapture_pre_autograd_graph\u001b[0;34m(f, args, kwargs, dynamic_shapes)\u001b[0m\n\u001b[1;32m    145\u001b[0m decomp_table \u001b[39m=\u001b[39m {\n\u001b[1;32m    146\u001b[0m     op: op\u001b[39m.\u001b[39mdecompose\n\u001b[1;32m    147\u001b[0m     \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m FunctionalTensor\u001b[39m.\u001b[39mmaybe_aliasing_or_mutating_ops\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m op \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39maten\u001b[39m.\u001b[39mdropout\u001b[39m.\u001b[39mdefault\n\u001b[1;32m    149\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpatch(dataclasses\u001b[39m.\u001b[39masdict(DEFAULT_EXPORT_DYNAMO_CONFIG)):\n\u001b[0;32m--> 151\u001b[0m     m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m    152\u001b[0m         f,\n\u001b[1;32m    153\u001b[0m         constraints\u001b[39m=\u001b[39;49mconstraints,\n\u001b[1;32m    154\u001b[0m         assume_static_by_default\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    155\u001b[0m         tracing_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msymbolic\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    156\u001b[0m         decomposition_table\u001b[39m=\u001b[39;49mdecomp_table,\n\u001b[1;32m    157\u001b[0m         pre_dispatch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    158\u001b[0m         aten_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    159\u001b[0m         _log_export_usage\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    160\u001b[0m     )(\n\u001b[1;32m    161\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    162\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    163\u001b[0m     )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    165\u001b[0m     _, _, _, fake_mode \u001b[39m=\u001b[39m _convert_input_to_fake(m, args, kwargs)\n\u001b[1;32m    167\u001b[0m     m\u001b[39m.\u001b[39mmeta[\u001b[39m\"\u001b[39m\u001b[39minline_constraints\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\n\u001b[1;32m    168\u001b[0m         k: v\n\u001b[1;32m    169\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m fake_mode\u001b[39m.\u001b[39mshape_env\u001b[39m.\u001b[39mvar_to_range\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39mmatch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m^[if]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+$\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(k))\n\u001b[1;32m    171\u001b[0m     }\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1311\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[39m# TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1311\u001b[0m     result_traced \u001b[39m=\u001b[39m opt_f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1312\u001b[0m \u001b[39mexcept\u001b[39;00m ConstraintViolationError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1313\u001b[0m     constraint_violation_error \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[39m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    452\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:921\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[1;32m    919\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m    920\u001b[0m     \u001b[39m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_entry, hooks, frame_state, skip\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:400\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    386\u001b[0m compile_id \u001b[39m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    388\u001b[0m signpost_event(\n\u001b[1;32m    389\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdynamo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    390\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_convert_frame_assert._compile\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     },\n\u001b[1;32m    398\u001b[0m )\n\u001b[0;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    401\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    402\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    403\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    404\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    405\u001b[0m     compiler_fn,\n\u001b[1;32m    406\u001b[0m     one_graph,\n\u001b[1;32m    407\u001b[0m     export,\n\u001b[1;32m    408\u001b[0m     export_constraints,\n\u001b[1;32m    409\u001b[0m     hooks,\n\u001b[1;32m    410\u001b[0m     cache_size,\n\u001b[1;32m    411\u001b[0m     frame,\n\u001b[1;32m    412\u001b[0m     frame_state\u001b[39m=\u001b[39;49mframe_state,\n\u001b[1;32m    413\u001b[0m     compile_id\u001b[39m=\u001b[39;49mcompile_id,\n\u001b[1;32m    414\u001b[0m     skip\u001b[39m=\u001b[39;49mskip \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    415\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:676\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    674\u001b[0m fail_user_frame_lineno: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 676\u001b[0m     guarded_code \u001b[39m=\u001b[39m compile_inner(code, one_graph, hooks, transform)\n\u001b[1;32m    677\u001b[0m     \u001b[39mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    678\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m    679\u001b[0m     Unsupported,\n\u001b[1;32m    680\u001b[0m     TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m     BisectValidationException,\n\u001b[1;32m    688\u001b[0m ) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m (dynamo_timed)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[39m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:535\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    533\u001b[0m CompileContext\u001b[39m.\u001b[39mget()\u001b[39m.\u001b[39mattempt \u001b[39m=\u001b[39m attempt\n\u001b[1;32m    534\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m     out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    536\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mRestartAnalysis \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1036\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1033\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1034\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1036\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m   1037\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:165\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m cleanup \u001b[39m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     cleanup\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:500\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[39mwith\u001b[39;00m tracing(tracer\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mtracing_context), tracer\u001b[39m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 500\u001b[0m         tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    502\u001b[0m     speculation_log\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2149\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2149\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    807\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 810\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    811\u001b[0m     ):\n\u001b[1;32m    812\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     TracingContext\u001b[39m.\u001b[39mset_current_loc(\n\u001b[1;32m    771\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 773\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    775\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:489\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     TracingContext\u001b[39m.\u001b[39mset_current_loc(\n\u001b[1;32m    487\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[1;32m    488\u001b[0m     )\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_fn(\u001b[39mself\u001b[39;49m, inst)\n\u001b[1;32m    490\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported \u001b[39mas\u001b[39;00m excp:\n\u001b[1;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_context_manager_depth \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    492\u001b[0m         \u001b[39m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    493\u001b[0m         \u001b[39m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1272\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_KW\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1270\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(argnames, kwargs_list))\n\u001b[1;32m   1271\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(argnames)\n\u001b[0;32m-> 1272\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_function(fn, args, kwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:674\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m inner_fn \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(inner_fn) \u001b[39mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    673\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempt to trace forbidden callable \u001b[39m\u001b[39m{\u001b[39;00minner_fn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush(fn\u001b[39m.\u001b[39;49mcall_function(\u001b[39mself\u001b[39;49m, args, kwargs))\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:336\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[39massert\u001b[39;00m istype(fn, types\u001b[39m.\u001b[39mFunctionType)\n\u001b[0;32m--> 336\u001b[0m \u001b[39mreturn\u001b[39;00m tx\u001b[39m.\u001b[39;49minline_user_function_return(\n\u001b[1;32m    337\u001b[0m     variables\u001b[39m.\u001b[39;49mUserFunctionVariable(fn, source\u001b[39m=\u001b[39;49mfn_source),\n\u001b[1;32m    338\u001b[0m     args,\n\u001b[1;32m    339\u001b[0m     kwargs,\n\u001b[1;32m    340\u001b[0m )\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:680\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minline_user_function_return\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mreturn\u001b[39;00m InliningInstructionTranslator\u001b[39m.\u001b[39;49minline_call(\u001b[39mself\u001b[39;49m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2285\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   2283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minline_call\u001b[39m(\u001b[39mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2284\u001b[0m     \u001b[39mwith\u001b[39;00m patch\u001b[39m.\u001b[39mdict(counters, {\u001b[39m\"\u001b[39m\u001b[39munimplemented\u001b[39m\u001b[39m\"\u001b[39m: counters[\u001b[39m\"\u001b[39m\u001b[39minline_call\u001b[39m\u001b[39m\"\u001b[39m]}):\n\u001b[0;32m-> 2285\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49minline_call_(parent, func, args, kwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2399\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[39mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2399\u001b[0m         tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   2400\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mSkipFrame \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2401\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSKIPPED INLINING \u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    807\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 810\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    811\u001b[0m     ):\n\u001b[1;32m    812\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     TracingContext\u001b[39m.\u001b[39mset_current_loc(\n\u001b[1;32m    771\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 773\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    775\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:489\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     TracingContext\u001b[39m.\u001b[39mset_current_loc(\n\u001b[1;32m    487\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[1;32m    488\u001b[0m     )\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_fn(\u001b[39mself\u001b[39;49m, inst)\n\u001b[1;32m    490\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported \u001b[39mas\u001b[39;00m excp:\n\u001b[1;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_context_manager_depth \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    492\u001b[0m         \u001b[39m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    493\u001b[0m         \u001b[39m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1260\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[39m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m kwargsvars \u001b[39m=\u001b[39m kwargsvars\u001b[39m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 1260\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_function(fn, argsvars\u001b[39m.\u001b[39;49mitems, kwargsvars)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:674\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m inner_fn \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(inner_fn) \u001b[39mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    673\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempt to trace forbidden callable \u001b[39m\u001b[39m{\u001b[39;00minner_fn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush(fn\u001b[39m.\u001b[39;49mcall_function(\u001b[39mself\u001b[39;49m, args, kwargs))\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:335\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    328\u001b[0m         module_attr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         \u001b[39mand\u001b[39;00m module_attr\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtorch.nn.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_constant\n\u001b[1;32m    331\u001b[0m     ):\n\u001b[1;32m    332\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mcall_method(\n\u001b[1;32m    333\u001b[0m             tx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, args, kwargs, constant\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_constant\n\u001b[1;32m    334\u001b[0m         )\n\u001b[0;32m--> 335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:289\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_constant:\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    286\u001b[0m         tx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    287\u001b[0m     )\n\u001b[0;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcall_function(tx, args, kwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m, tx, args: \u001b[39m\"\u001b[39m\u001b[39mList[VariableTracker]\u001b[39m\u001b[39m\"\u001b[39m, kwargs: \u001b[39m\"\u001b[39m\u001b[39mDict[str, VariableTracker]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mVariableTracker\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m tx\u001b[39m.\u001b[39;49minline_user_function_return(\n\u001b[1;32m     91\u001b[0m         \u001b[39mself\u001b[39;49m, \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_args()) \u001b[39m+\u001b[39;49m \u001b[39mlist\u001b[39;49m(args), kwargs\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:680\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minline_user_function_return\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mreturn\u001b[39;00m InliningInstructionTranslator\u001b[39m.\u001b[39;49minline_call(\u001b[39mself\u001b[39;49m, fn, args, kwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2285\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   2283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minline_call\u001b[39m(\u001b[39mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2284\u001b[0m     \u001b[39mwith\u001b[39;00m patch\u001b[39m.\u001b[39mdict(counters, {\u001b[39m\"\u001b[39m\u001b[39munimplemented\u001b[39m\u001b[39m\"\u001b[39m: counters[\u001b[39m\"\u001b[39m\u001b[39minline_call\u001b[39m\u001b[39m\"\u001b[39m]}):\n\u001b[0;32m-> 2285\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49minline_call_(parent, func, args, kwargs)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2399\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[39mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2399\u001b[0m         tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   2400\u001b[0m \u001b[39mexcept\u001b[39;00m exc\u001b[39m.\u001b[39mSkipFrame \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2401\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSKIPPED INLINING \u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    807\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 810\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    811\u001b[0m     ):\n\u001b[1;32m    812\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     TracingContext\u001b[39m.\u001b[39mset_current_loc(\n\u001b[1;32m    771\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 773\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    775\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:489\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     TracingContext\u001b[39m.\u001b[39mset_current_loc(\n\u001b[1;32m    487\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\n\u001b[1;32m    488\u001b[0m     )\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_fn(\u001b[39mself\u001b[39;49m, inst)\n\u001b[1;32m    490\u001b[0m \u001b[39mexcept\u001b[39;00m Unsupported \u001b[39mas\u001b[39;00m excp:\n\u001b[1;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_context_manager_depth \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    492\u001b[0m         \u001b[39m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    493\u001b[0m         \u001b[39m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1219\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1217\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopn(inst\u001b[39m.\u001b[39margval)\n\u001b[1;32m   1218\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpop()\n\u001b[0;32m-> 1219\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_function(fn, args, {})\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:674\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m inner_fn \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(inner_fn) \u001b[39mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    673\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempt to trace forbidden callable \u001b[39m\u001b[39m{\u001b[39;00minner_fn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush(fn\u001b[39m.\u001b[39;49mcall_function(\u001b[39mself\u001b[39;49m, args, kwargs))\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/variables/base.py:349\u001b[0m, in \u001b[0;36mVariableTracker.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_function\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m, tx, args: \u001b[39m\"\u001b[39m\u001b[39mList[VariableTracker]\u001b[39m\u001b[39m\"\u001b[39m, kwargs: \u001b[39m\"\u001b[39m\u001b[39mDict[str, VariableTracker]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mVariableTracker\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 349\u001b[0m     unimplemented(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcall_function \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m{\u001b[39;49;00margs\u001b[39m}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m{\u001b[39;49;00mkwargs\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/env/tmp/lib/python3.10/site-packages/torch/_dynamo/exc.py:190\u001b[0m, in \u001b[0;36munimplemented\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munimplemented\u001b[39m(msg: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m    189\u001b[0m     \u001b[39massert\u001b[39;00m msg \u001b[39m!=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mBREAK\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mraise\u001b[39;00m Unsupported(msg)\n",
      "\u001b[0;31mUnsupported\u001b[0m: call_function ConstantVariable(int: 128) [] {}\n\nfrom user code:\n   File \"/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1564, in forward\n    outputs = self.bert(\n  File \"/home/dlyakhov/env/tmp/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 961, in forward\n    input_shape = input_ids.size()\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize(torch_model, calibration_dataset.get_inference_data())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e082b01d",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Post-training Quantization API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "We will use 8-bit quantization in post-training mode (without the fine-tuning pipeline) to optimize BERT.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a Dataset for quantization\n",
    "2. Run `nncf.quantize` for getting an optimized model\n",
    "3. Serialize OpenVINO IR model using `openvino.save_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e089ea99",
   "metadata": {
    "test_replace": {
     "quantized_model = nncf.quantize(model, calibration_dataset, model_type=ModelType.TRANSFORMER)": "quantized_model = nncf.quantize(model, calibration_dataset, model_type=ModelType.TRANSFORMER, subset_size=10)"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:202 ignored nodes was found by types in the NNCFGraph\n",
      "INFO:nncf:24 ignored nodes was found by name in the NNCFGraph\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 22 aten::rsub_16\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 25 aten::rsub_17\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 30 aten::mul_18\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 11 aten::add_40\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 14 aten::add__46\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 17 aten::layer_norm_48\n",
      "20 aten::layer_norm_49\n",
      "23 aten::layer_norm_50\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 36 aten::add_108\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 55 aten::softmax_109\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 74 aten::matmul_110\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 26 aten::add_126\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 31 aten::layer_norm_128\n",
      "47 aten::layer_norm_129\n",
      "66 aten::layer_norm_130\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 85 aten::add_140\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 103 aten::layer_norm_142\n",
      "133 aten::layer_norm_143\n",
      "171 aten::layer_norm_144\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 38 aten::add_202\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 57 aten::softmax_203\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 76 aten::matmul_204\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 209 aten::add_220\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 236 aten::layer_norm_222\n",
      "250 aten::layer_norm_223\n",
      "267 aten::layer_norm_224\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 287 aten::add_234\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 316 aten::layer_norm_236\n",
      "342 aten::layer_norm_237\n",
      "364 aten::layer_norm_238\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 39 aten::add_296\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 58 aten::softmax_297\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 77 aten::matmul_298\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 221 aten::add_314\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 242 aten::layer_norm_316\n",
      "259 aten::layer_norm_317\n",
      "279 aten::layer_norm_318\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 300 aten::add_328\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 326 aten::layer_norm_330\n",
      "348 aten::layer_norm_331\n",
      "370 aten::layer_norm_332\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 40 aten::add_390\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 59 aten::softmax_391\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 78 aten::matmul_392\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 223 aten::add_408\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 243 aten::layer_norm_410\n",
      "260 aten::layer_norm_411\n",
      "280 aten::layer_norm_412\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 302 aten::add_422\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 328 aten::layer_norm_424\n",
      "350 aten::layer_norm_425\n",
      "372 aten::layer_norm_426\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 41 aten::add_484\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 60 aten::softmax_485\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 79 aten::matmul_486\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 225 aten::add_502\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 244 aten::layer_norm_504\n",
      "261 aten::layer_norm_505\n",
      "281 aten::layer_norm_506\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 304 aten::add_516\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 330 aten::layer_norm_518\n",
      "352 aten::layer_norm_519\n",
      "374 aten::layer_norm_520\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 42 aten::add_578\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 61 aten::softmax_579\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 80 aten::matmul_580\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 227 aten::add_596\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 245 aten::layer_norm_598\n",
      "262 aten::layer_norm_599\n",
      "282 aten::layer_norm_600\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 306 aten::add_610\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 332 aten::layer_norm_612\n",
      "354 aten::layer_norm_613\n",
      "376 aten::layer_norm_614\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 43 aten::add_672\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 62 aten::softmax_673\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 81 aten::matmul_674\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 229 aten::add_690\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 246 aten::layer_norm_692\n",
      "263 aten::layer_norm_693\n",
      "283 aten::layer_norm_694\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 308 aten::add_704\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 334 aten::layer_norm_706\n",
      "356 aten::layer_norm_707\n",
      "378 aten::layer_norm_708\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 44 aten::add_766\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 63 aten::softmax_767\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 82 aten::matmul_768\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 231 aten::add_784\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 247 aten::layer_norm_786\n",
      "264 aten::layer_norm_787\n",
      "284 aten::layer_norm_788\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 310 aten::add_798\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 336 aten::layer_norm_800\n",
      "358 aten::layer_norm_801\n",
      "380 aten::layer_norm_802\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 45 aten::add_860\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 64 aten::softmax_861\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 83 aten::matmul_862\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 233 aten::add_878\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 248 aten::layer_norm_880\n",
      "265 aten::layer_norm_881\n",
      "285 aten::layer_norm_882\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 312 aten::add_892\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 338 aten::layer_norm_894\n",
      "360 aten::layer_norm_895\n",
      "382 aten::layer_norm_896\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 46 aten::add_954\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 65 aten::softmax_955\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 84 aten::matmul_956\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 235 aten::add_972\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 249 aten::layer_norm_974\n",
      "266 aten::layer_norm_975\n",
      "286 aten::layer_norm_976\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 314 aten::add_986\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 340 aten::layer_norm_988\n",
      "362 aten::layer_norm_989\n",
      "384 aten::layer_norm_990\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 35 aten::add_1048\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 54 aten::softmax_1049\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 73 aten::matmul_1050\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 215 aten::add_1066\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 240 aten::layer_norm_1068\n",
      "257 aten::layer_norm_1069\n",
      "277 aten::layer_norm_1070\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 296 aten::add_1080\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 322 aten::layer_norm_1082\n",
      "344 aten::layer_norm_1083\n",
      "366 aten::layer_norm_1084\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 37 aten::add_1142\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 56 aten::softmax_1143\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 75 aten::matmul_1144\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 218 aten::add_1160\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 241 aten::layer_norm_1162\n",
      "258 aten::layer_norm_1163\n",
      "278 aten::layer_norm_1164\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 298 aten::add_1174\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 324 aten::layer_norm_1176\n",
      "346 aten::layer_norm_1177\n",
      "368 aten::layer_norm_1178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics collection: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:26<00:00, 11.28it/s]\n",
      "Biases correction: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:25<00:00,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "INPUT_NAMES = [key for key in inputs.keys()]\n",
    "\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    \"\"\"\n",
    "    Extract the model's input from the data item.\n",
    "    The data item here is the data item that is returned from the data source per iteration.\n",
    "    This function should be passed when the data item cannot be used as model's input.\n",
    "    \"\"\"\n",
    "    inputs = {name: np.asarray([data_item[name]], dtype=np.int64) for name in INPUT_NAMES}\n",
    "    return inputs\n",
    "\n",
    "\n",
    "calibration_dataset = nncf.Dataset(data_source, transform_fn)\n",
    "# Quantize the model. By specifying model_type, we specify additional transformer patterns in the model.\n",
    "quantized_model = nncf.quantize(model, calibration_dataset, model_type=ModelType.TRANSFORMER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da83574c-7abc-40a8-ae30-431c1b2bd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model_xml = Path(MODEL_DIR) / \"quantized_bert_mrpc.xml\"\n",
    "ov.save_model(quantized_model, compressed_model_xml)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c30ab44",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and Test OpenVINO Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To load and test converted model, perform the following:\n",
    "\n",
    "* Load the model and compile it for selected device.\n",
    "* Prepare the input.\n",
    "* Run the inference.\n",
    "* Get the answer from the model output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93749c47-073f-4ffe-a507-4d38447159f5",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6436bcf3-e446-4fee-a6ed-58235119a18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdd9d273ae64886993d433f91a7289a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d79b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model for a specific device.\n",
    "compiled_quantized_model = core.compile_model(model=quantized_model, device_name=device.value)\n",
    "output_layer = compiled_quantized_model.outputs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef1d846e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Data Source returns a pair of sentences (indicated by `sample_idx`) and the inference compares these sentences and outputs whether their meaning is the same. You can test other sentences by changing `sample_idx` to another value (from 0 to 407)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e72504b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: Wal-Mart said it would check all of its million-plus domestic workers to ensure they were legally employed .\n",
      "Text 2: It has also said it would review all of its domestic employees more than 1 million to ensure they have legal status .\n",
      "The same meaning: yes\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 5\n",
    "sample = data_source[sample_idx]\n",
    "inputs = {k: torch.unsqueeze(torch.tensor(sample[k]), 0) for k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "\n",
    "result = compiled_quantized_model(inputs)[output_layer]\n",
    "result = np.argmax(result)\n",
    "\n",
    "print(f\"Text 1: {sample['sentence1']}\")\n",
    "print(f\"Text 2: {sample['sentence2']}\")\n",
    "print(f\"The same meaning: {'yes' if result == 1 else 'no'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89920c37-dc2f-4177-b25f-bd8b1d0e34d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare F1-score of FP32 and INT8 models\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeea7cc8-3eed-4474-8f59-ae63197368d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the accuracy of the original model:\n",
      "F1 score: 0.9019\n",
      "Checking the accuracy of the quantized model:\n",
      "F1 score: 0.8995\n"
     ]
    }
   ],
   "source": [
    "def validate(model: ov.Model, dataset: Iterable[Any]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on GLUE dataset.\n",
    "    Returns F1 score metric.\n",
    "    \"\"\"\n",
    "    compiled_model = core.compile_model(model, device_name=device.value)\n",
    "    output_layer = compiled_model.output(0)\n",
    "\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    for batch in dataset:\n",
    "        inputs = [np.expand_dims(np.asarray(batch[key], dtype=np.int64), 0) for key in INPUT_NAMES]\n",
    "        outputs = compiled_model(inputs)[output_layer]\n",
    "        predictions = outputs[0].argmax(axis=-1)\n",
    "        metric.add_batch(predictions=[predictions], references=[batch[\"labels\"]])\n",
    "    metrics = metric.compute()\n",
    "    f1_score = metrics[\"f1\"]\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "print(\"Checking the accuracy of the original model:\")\n",
    "metric = validate(model, data_source)\n",
    "print(f\"F1 score: {metric:.4f}\")\n",
    "\n",
    "print(\"Checking the accuracy of the quantized model:\")\n",
    "metric = validate(quantized_model, data_source)\n",
    "print(f\"F1 score: {metric:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f67f6a2",
   "metadata": {
    "id": "vQACMfAUo52V",
    "tags": []
   },
   "source": [
    "## Compare Performance of the Original, Converted and Quantized Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Compare the original PyTorch model with OpenVINO converted and quantized models (`FP32`, `INT8`) to see the difference in performance. It is expressed in Sentences Per Second (SPS) measure, which is the same as Frames Per Second (FPS) for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "734ae69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model for a specific device.\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f484fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model on CPU: 0.080 seconds per sentence, SPS: 12.47\n",
      "IR FP32 model in OpenVINO Runtime/AUTO: 0.024 seconds per sentence, SPS: 41.92\n",
      "OpenVINO IR INT8 model in OpenVINO Runtime/AUTO: 0.012 seconds per sentence, SPS: 84.38\n"
     ]
    }
   ],
   "source": [
    "num_samples = 50\n",
    "sample = data_source[0]\n",
    "inputs = {k: torch.unsqueeze(torch.tensor(sample[k]), 0) for k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_samples):\n",
    "        torch_model(torch.vstack(list(inputs.values())))\n",
    "    end = time.perf_counter()\n",
    "    time_torch = end - start\n",
    "print(f\"PyTorch model on CPU: {time_torch / num_samples:.3f} seconds per sentence, \" f\"SPS: {num_samples / time_torch:.2f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_samples):\n",
    "    compiled_model(inputs)\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(f\"IR FP32 model in OpenVINO Runtime/{device.value}: {time_ir / num_samples:.3f} \" f\"seconds per sentence, SPS: {num_samples / time_ir:.2f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_samples):\n",
    "    compiled_quantized_model(inputs)\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(f\"OpenVINO IR INT8 model in OpenVINO Runtime/{device.value}: {time_ir / num_samples:.3f} \" f\"seconds per sentence, SPS: {num_samples / time_ir:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "add78af0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, measure the inference performance of OpenVINO `FP32` and `INT8` models. For this purpose, use [Benchmark Tool](https://docs.openvino.ai/2024/learn-openvino/openvino-samples/benchmark-tool.html) in OpenVINO.\n",
    "\n",
    "> **Note**: The `benchmark_app` tool is able to measure the performance of the OpenVINO Intermediate Representation (OpenVINO IR) models only. For more accurate performance, run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71b38a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ WARNING ] Default duration 120 seconds is used for unknown device device.value\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2023.0.0-10926-b4452d56304-releases/2023/0\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ ERROR ] Check 'false' failed at src/inference/src/core.cpp:84:\n",
      "Device with \"device\" name is not registered in the OpenVINO Runtime\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ea/work/notebooks_convert/notebooks_conv_env/lib/python3.8/site-packages/openvino/tools/benchmark/main.py\", line 103, in main\n",
      "    benchmark.print_version_info()\n",
      "  File \"/home/ea/work/notebooks_convert/notebooks_conv_env/lib/python3.8/site-packages/openvino/tools/benchmark/benchmark.py\", line 48, in print_version_info\n",
      "    for device, version in self.core.get_versions(self.device).items():\n",
      "RuntimeError: Check 'false' failed at src/inference/src/core.cpp:84:\n",
      "Device with \"device\" name is not registered in the OpenVINO Runtime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference FP32 model (OpenVINO IR)\n",
    "!benchmark_app -m $ir_model_xml -shape [1,128],[1,128],[1,128] -d {device.value} -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf41525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ WARNING ] Default duration 120 seconds is used for unknown device device.value\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2023.0.0-10926-b4452d56304-releases/2023/0\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ ERROR ] Check 'false' failed at src/inference/src/core.cpp:84:\n",
      "Device with \"device\" name is not registered in the OpenVINO Runtime\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ea/work/notebooks_convert/notebooks_conv_env/lib/python3.8/site-packages/openvino/tools/benchmark/main.py\", line 103, in main\n",
      "    benchmark.print_version_info()\n",
      "  File \"/home/ea/work/notebooks_convert/notebooks_conv_env/lib/python3.8/site-packages/openvino/tools/benchmark/benchmark.py\", line 48, in print_version_info\n",
      "    for device, version in self.core.get_versions(self.device).items():\n",
      "RuntimeError: Check 'false' failed at src/inference/src/core.cpp:84:\n",
      "Device with \"device\" name is not registered in the OpenVINO Runtime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "! benchmark_app -m $compressed_model_xml -shape [1,128],[1,128],[1,128] -d {device.value} -api sync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "openvino_notebooks": {
   "imageUrl": "",
   "tags": {
    "categories": [
     "API Overview",
     "Optimize"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Text Classification"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
