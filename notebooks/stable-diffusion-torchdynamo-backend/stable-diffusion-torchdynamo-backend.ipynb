{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion v2.1 using OpenVINO TorchDynamo backend\n",
    "\n",
    "Stable Diffusion v2 is the next generation of Stable Diffusion model a Text-to-Image latent diffusion model created by the researchers and engineers from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). \n",
    "\n",
    "General diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image.\n",
    "Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference. OpenVINO brings capabilities to run model inference on Intel hardware and opens the door to the fantastic world of diffusion models for everyone!\n",
    "\n",
    "This notebook demonstrates how to run stable diffusion model using [Diffusers](https://huggingface.co/docs/diffusers/index) library and [OpenVINO `TorchDynamo` backend](https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html) for Text-to-Image and Image-to-Image generation tasks.\n",
    "\n",
    "Notebook contains the following steps:\n",
    "\n",
    "1. Create pipeline with PyTorch models.\n",
    "2. Add OpenVINO optimization using OpenVINO TorchDynamo backend.\n",
    "3. Run Stable Diffusion pipeline with OpenVINO.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Stable Diffusion with Diffusers library](#Stable-Diffusion-with-Diffusers-library)\n",
    "- [OpenVINO TorchDynamo backend](#OpenVINO-TorchDynamo-backend)\n",
    "    - [Run Image generation](#Run-Image-generation)\n",
    "- [Interactive demo](#Interactive-demo)\n",
    "- [Support for Automatic1111 Stable Diffusion WebUI](#Support-for-Automatic1111-Stable-Diffusion-WebUI)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"torch>=2.2\" transformers diffusers \"gradio>=4.19\" ipywidgets --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"openvino>=2024.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion with Diffusers library\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To work with Stable Diffusion v2.1, we will use Hugging Face Diffusers library. To experiment with Stable Diffusion models, Diffusers exposes the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) and [StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/using-diffusers/img2img) similar to the other [Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview). The code below demonstrates how to create the `StableDiffusionPipeline` using `stable-diffusion-2-1-base` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398963b36ba244fd8a0fc2ef0ad85504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "# Pipeline for text-to-image generation\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO TorchDynamo backend\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The [OpenVINO TorchDynamo backend](https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html) lets you enable [OpenVINO](https://docs.openvino.ai/2024/home.html) support for PyTorch models with minimal changes to the original PyTorch script. It speeds up PyTorch code by JIT-compiling it into optimized kernels. By default, Torch code runs in eager-mode, but with the use of torch.compile it goes through the following steps:\n",
    "1. Graph acquisition - the model is rewritten as blocks of subgraphs that are either:\n",
    "   - compiled by TorchDynamo and “flattened”,\n",
    "   - falling back to the eager-mode, due to unsupported Python constructs (like control-flow code).\n",
    "2. Graph lowering - all PyTorch operations are decomposed into their constituent kernels specific to the chosen backend.\n",
    "3. Graph compilation - the kernels call their corresponding low-level device-specific operations.\n",
    "\n",
    "Select device for inference and enable or disable saving the optimized model files to a hard drive, after the first application run. This makes them available for the following application executions, reducing the first-inference latency. Read more about available [Environment Variables options](https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html#options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9e3a89fd424cf0b66e71bf51e616b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU.0', 'GPU.1', 'GPU.2', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d593ec41d3d4aadade75455992b58e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model caching:', options=(True, False), value=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_caching = widgets.Dropdown(\n",
    "    options=[True, False],\n",
    "    value=True,\n",
    "    description=\"Model caching:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_caching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use [`torch.compile()` method](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html), you just need to add an import statement and define the OpenVINO backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\n",
    "from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer\n",
    "from torch.ao.quantization.quantize_pt2e import convert_pt2e\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_pt2e\n",
    "from torch.export import Dim\n",
    "from itertools import islice\n",
    "\n",
    "constraint = Dim(\"ids_att_mask_const\", max=76)\n",
    "dynamic_shapes = {\"input_ids\": {1: constraint}, \"attention_mask\": {1: constraint},\n",
    "                  \"pixel_values\": {}}\n",
    "\n",
    "def quantize(model, calibration_dataset, subset_size: int = 300):\n",
    "    example_args = next(iter(calibration_dataset))\n",
    "    with torch.no_grad():\n",
    "        # Static\n",
    "        exported_model = capture_pre_autograd_graph(model, args=example_args, kwargs={})\n",
    "        # Dynamic\n",
    "        #exported_model = capture_pre_autograd_graph(model, args=(), kwargs=example_kwargs,\n",
    "        #                                            dynamic_shapes=dynamic_shapes)\n",
    "        # torch 2.3.1\n",
    "        #exported_model = capture_pre_autograd_graph(model, args=(), kwargs=example_kwargs, constraints=[\n",
    "        #    dynamic_dim(example_kwargs[\"input_ids\"], 1) == dynamic_dim(example_kwargs[\"attention_mask\"], 1),\n",
    "        #    # Torch.fx said I need this  constraint as well 0-0\n",
    "        #    dynamic_dim(example_kwargs[\"input_ids\"], 1) < 77,\n",
    "        #])\n",
    "\n",
    "    open(\"c.py\", \"w\").write(exported_model.code)\n",
    "    quantizer = X86InductorQuantizer()\n",
    "    quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n",
    "\n",
    "    prepared_model = prepare_pt2e(exported_model, quantizer)\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for inp  in islice(tqdm(calibration_dataset), subset_size):\n",
    "        # Static\n",
    "        # prepared_model(**example_kwargs)\n",
    "        # Dynamic\n",
    "        prepared_model(*inp)\n",
    "    converted_model = convert_pt2e(prepared_model)\n",
    "    return converted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "example_input = [(torch.ones(2, 4, 64, 64), 981, torch.ones(2, 77, 1024))]\n",
    "unet_quantized = quantize(pipe.unet, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DConditionOutput(sample=tensor([[[[0.2550, 0.3102, 0.2675,  ..., 0.2527, 0.3319, 0.4627],\n",
       "          [0.1833, 0.2141, 0.1447,  ..., 0.1216, 0.2257, 0.4241],\n",
       "          [0.1452, 0.1510, 0.0921,  ..., 0.0664, 0.1770, 0.3639],\n",
       "          ...,\n",
       "          [0.1341, 0.1622, 0.1039,  ..., 0.0903, 0.2078, 0.3962],\n",
       "          [0.2218, 0.2442, 0.1896,  ..., 0.1797, 0.2569, 0.4396],\n",
       "          [0.3356, 0.2939, 0.2538,  ..., 0.2442, 0.3156, 0.4656]],\n",
       "\n",
       "         [[0.3784, 0.1833, 0.1533,  ..., 0.1446, 0.1966, 0.0815],\n",
       "          [0.3786, 0.1726, 0.1422,  ..., 0.1422, 0.1735, 0.0477],\n",
       "          [0.3865, 0.1689, 0.1423,  ..., 0.1444, 0.1889, 0.0547],\n",
       "          ...,\n",
       "          [0.3688, 0.1519, 0.1203,  ..., 0.1325, 0.1775, 0.0477],\n",
       "          [0.3627, 0.1491, 0.1245,  ..., 0.1278, 0.1777, 0.0442],\n",
       "          [0.4313, 0.2987, 0.2745,  ..., 0.2742, 0.3371, 0.1511]],\n",
       "\n",
       "         [[0.6024, 0.4573, 0.4148,  ..., 0.4265, 0.4101, 0.5287],\n",
       "          [0.3853, 0.1774, 0.1664,  ..., 0.1485, 0.1382, 0.3070],\n",
       "          [0.4011, 0.1790, 0.1647,  ..., 0.1521, 0.1539, 0.2985],\n",
       "          ...,\n",
       "          [0.4026, 0.1969, 0.1753,  ..., 0.1576, 0.1443, 0.3071],\n",
       "          [0.3824, 0.1764, 0.1659,  ..., 0.1327, 0.1288, 0.3010],\n",
       "          [0.4643, 0.2796, 0.2657,  ..., 0.2550, 0.2373, 0.3351]],\n",
       "\n",
       "         [[0.2684, 0.1942, 0.2004,  ..., 0.2110, 0.2499, 0.4199],\n",
       "          [0.2865, 0.1399, 0.1530,  ..., 0.1461, 0.1850, 0.4034],\n",
       "          [0.2781, 0.1191, 0.1141,  ..., 0.1190, 0.1664, 0.3926],\n",
       "          ...,\n",
       "          [0.2703, 0.1135, 0.1131,  ..., 0.1121, 0.1495, 0.3976],\n",
       "          [0.3158, 0.1716, 0.1611,  ..., 0.1604, 0.2053, 0.4261],\n",
       "          [0.5310, 0.3913, 0.3971,  ..., 0.3979, 0.4504, 0.5087]]],\n",
       "\n",
       "\n",
       "        [[[0.2550, 0.3102, 0.2675,  ..., 0.2527, 0.3319, 0.4627],\n",
       "          [0.1833, 0.2141, 0.1447,  ..., 0.1216, 0.2257, 0.4241],\n",
       "          [0.1452, 0.1510, 0.0921,  ..., 0.0664, 0.1770, 0.3639],\n",
       "          ...,\n",
       "          [0.1341, 0.1622, 0.1039,  ..., 0.0903, 0.2078, 0.3962],\n",
       "          [0.2218, 0.2442, 0.1896,  ..., 0.1797, 0.2569, 0.4396],\n",
       "          [0.3356, 0.2939, 0.2538,  ..., 0.2442, 0.3156, 0.4656]],\n",
       "\n",
       "         [[0.3784, 0.1833, 0.1533,  ..., 0.1446, 0.1966, 0.0815],\n",
       "          [0.3786, 0.1726, 0.1422,  ..., 0.1422, 0.1735, 0.0477],\n",
       "          [0.3865, 0.1689, 0.1423,  ..., 0.1444, 0.1889, 0.0547],\n",
       "          ...,\n",
       "          [0.3688, 0.1519, 0.1203,  ..., 0.1325, 0.1775, 0.0477],\n",
       "          [0.3627, 0.1491, 0.1245,  ..., 0.1278, 0.1777, 0.0442],\n",
       "          [0.4313, 0.2987, 0.2745,  ..., 0.2742, 0.3371, 0.1511]],\n",
       "\n",
       "         [[0.6024, 0.4573, 0.4148,  ..., 0.4265, 0.4101, 0.5287],\n",
       "          [0.3853, 0.1774, 0.1664,  ..., 0.1485, 0.1382, 0.3070],\n",
       "          [0.4011, 0.1790, 0.1647,  ..., 0.1521, 0.1539, 0.2985],\n",
       "          ...,\n",
       "          [0.4026, 0.1969, 0.1753,  ..., 0.1576, 0.1443, 0.3071],\n",
       "          [0.3824, 0.1764, 0.1659,  ..., 0.1327, 0.1288, 0.3010],\n",
       "          [0.4643, 0.2796, 0.2657,  ..., 0.2550, 0.2373, 0.3351]],\n",
       "\n",
       "         [[0.2684, 0.1942, 0.2004,  ..., 0.2110, 0.2499, 0.4199],\n",
       "          [0.2865, 0.1399, 0.1530,  ..., 0.1461, 0.1850, 0.4034],\n",
       "          [0.2781, 0.1191, 0.1141,  ..., 0.1190, 0.1664, 0.3926],\n",
       "          ...,\n",
       "          [0.2703, 0.1135, 0.1131,  ..., 0.1121, 0.1495, 0.3976],\n",
       "          [0.3158, 0.1716, 0.1611,  ..., 0.1604, 0.2053, 0.4261],\n",
       "          [0.5310, 0.3913, 0.3971,  ..., 0.3979, 0.4504, 0.5087]]]],\n",
       "       grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_quantized(*example_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this import is required to activate the openvino backend for torchdynamo\n",
    "import openvino.torch  # noqa: F401\n",
    "\n",
    "pipe.unet = torch.compile(\n",
    "    pipe.unet,\n",
    "    backend=\"openvino\",\n",
    "    options={\"device\": device.value, \"model_caching\": model_caching.value},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Read more about available [OpenVINO backends](https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html#how-to-use)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Image generation\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlyakhov/env/tmp/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:222: FutureWarning: Accessing config attribute `__iter__` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access '__iter__' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.__iter__'.\n",
      "  return getattr(self._orig_mod, name)\n",
      "OpenVINO execution failed. Falling back to native PyTorch execution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce107af9a034ec2a1a5b8aca6c4ddc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now you can start the demo, choose the inference mode, define prompts (and input image for Image-to-Image generation) and run inference pipeline.\n",
    "Optionally, you can also change some input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "time_stamps = []\n",
    "\n",
    "\n",
    "def callback(iter, t, latents):\n",
    "    time_stamps.append(time.time())\n",
    "\n",
    "\n",
    "def error_str(error, title=\"Error\"):\n",
    "    return (\n",
    "        f\"\"\"#### {title}\n",
    "            {error}\"\"\"\n",
    "        if error\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def on_mode_change(mode):\n",
    "    return gr.update(visible=mode == modes[\"img2img\"]), gr.update(visible=mode == modes[\"txt2img\"])\n",
    "\n",
    "\n",
    "def inference(\n",
    "    inf_mode,\n",
    "    prompt,\n",
    "    guidance=7.5,\n",
    "    steps=25,\n",
    "    width=768,\n",
    "    height=768,\n",
    "    seed=-1,\n",
    "    img=None,\n",
    "    strength=0.5,\n",
    "    neg_prompt=\"\",\n",
    "):\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 10000000)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    res = None\n",
    "\n",
    "    global time_stamps, pipe\n",
    "    time_stamps = []\n",
    "    try:\n",
    "        if inf_mode == modes[\"txt2img\"]:\n",
    "            if type(pipe).__name__ != \"StableDiffusionPipeline\":\n",
    "                pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "                pipe.unet = torch.compile(pipe.unet, backend=\"openvino\")\n",
    "            res = pipe(\n",
    "                prompt,\n",
    "                negative_prompt=neg_prompt,\n",
    "                num_inference_steps=int(steps),\n",
    "                guidance_scale=guidance,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                generator=generator,\n",
    "                callback=callback,\n",
    "                callback_steps=1,\n",
    "            ).images\n",
    "        elif inf_mode == modes[\"img2img\"]:\n",
    "            if img is None:\n",
    "                return (\n",
    "                    None,\n",
    "                    None,\n",
    "                    gr.update(\n",
    "                        visible=True,\n",
    "                        value=error_str(\"Image is required for Image to Image mode\"),\n",
    "                    ),\n",
    "                )\n",
    "            if type(pipe).__name__ != \"StableDiffusionImg2ImgPipeline\":\n",
    "                pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "                pipe.unet = torch.compile(pipe.unet, backend=\"openvino\")\n",
    "            res = pipe(\n",
    "                prompt,\n",
    "                negative_prompt=neg_prompt,\n",
    "                image=img,\n",
    "                num_inference_steps=int(steps),\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance,\n",
    "                generator=generator,\n",
    "                callback=callback,\n",
    "                callback_steps=1,\n",
    "            ).images\n",
    "    except Exception as e:\n",
    "        return None, None, gr.update(visible=True, value=error_str(e))\n",
    "\n",
    "    warmup_duration = time_stamps[1] - time_stamps[0]\n",
    "    generation_rate = (steps - 1) / (time_stamps[-1] - time_stamps[1])\n",
    "    res_info = \"Warm up time: \" + str(round(warmup_duration, 2)) + \" secs \"\n",
    "    if generation_rate >= 1.0:\n",
    "        res_info = res_info + \", Performance: \" + str(round(generation_rate, 2)) + \" it/s \"\n",
    "    else:\n",
    "        res_info = res_info + \", Performance: \" + str(round(1 / generation_rate, 2)) + \" s/it \"\n",
    "\n",
    "    return (\n",
    "        res,\n",
    "        gr.update(visible=True, value=res_info),\n",
    "        gr.update(visible=False, value=None),\n",
    "    )\n",
    "\n",
    "\n",
    "modes = {\n",
    "    \"txt2img\": \"Text to Image\",\n",
    "    \"img2img\": \"Image to Image\",\n",
    "}\n",
    "\n",
    "with gr.Blocks(css=\"style.css\") as demo:\n",
    "    gr.HTML(\n",
    "        f\"\"\"\n",
    "            Model used: {model_id}         \n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=60):\n",
    "            with gr.Group():\n",
    "                prompt = gr.Textbox(\n",
    "                    \"a photograph of an astronaut riding a horse\",\n",
    "                    label=\"Prompt\",\n",
    "                    max_lines=2,\n",
    "                )\n",
    "                neg_prompt = gr.Textbox(\n",
    "                    \"frames, borderline, text, character, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\",\n",
    "                    label=\"Negative prompt\",\n",
    "                )\n",
    "                res_img = gr.Gallery(label=\"Generated images\", show_label=False)\n",
    "            error_output = gr.Markdown(visible=False)\n",
    "\n",
    "        with gr.Column(scale=40):\n",
    "            generate = gr.Button(value=\"Generate\")\n",
    "\n",
    "            with gr.Group():\n",
    "                inf_mode = gr.Dropdown(list(modes.values()), label=\"Inference Mode\", value=modes[\"txt2img\"])\n",
    "\n",
    "                with gr.Column(visible=False) as i2i:\n",
    "                    image = gr.Image(label=\"Image\", height=128, type=\"pil\")\n",
    "                    strength = gr.Slider(\n",
    "                        label=\"Transformation strength\",\n",
    "                        minimum=0,\n",
    "                        maximum=1,\n",
    "                        step=0.01,\n",
    "                        value=0.5,\n",
    "                    )\n",
    "\n",
    "            with gr.Group():\n",
    "                with gr.Row() as txt2i:\n",
    "                    width = gr.Slider(label=\"Width\", value=512, minimum=64, maximum=1024, step=8)\n",
    "                    height = gr.Slider(label=\"Height\", value=512, minimum=64, maximum=1024, step=8)\n",
    "\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    steps = gr.Slider(label=\"Steps\", value=20, minimum=1, maximum=50, step=1)\n",
    "                    guidance = gr.Slider(label=\"Guidance scale\", value=7.5, maximum=15)\n",
    "\n",
    "                seed = gr.Slider(-1, 10000000, label=\"Seed (-1 = random)\", value=-1, step=1)\n",
    "\n",
    "            res_info = gr.Markdown(visible=False)\n",
    "\n",
    "    inf_mode.change(on_mode_change, inputs=[inf_mode], outputs=[i2i, txt2i], queue=False)\n",
    "\n",
    "    inputs = [\n",
    "        inf_mode,\n",
    "        prompt,\n",
    "        guidance,\n",
    "        steps,\n",
    "        width,\n",
    "        height,\n",
    "        seed,\n",
    "        image,\n",
    "        strength,\n",
    "        neg_prompt,\n",
    "    ]\n",
    "\n",
    "    outputs = [res_img, res_info, error_output]\n",
    "    prompt.submit(inference, inputs=inputs, outputs=outputs)\n",
    "    generate.click(inference, inputs=inputs, outputs=outputs)\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True, debug=True)\n",
    "\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for Automatic1111 Stable Diffusion WebUI\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Automatic1111 Stable Diffusion WebUI is an open-source repository that hosts a browser-based interface for the Stable Diffusion based image generation. It allows users to create realistic and creative images from text prompts. Stable Diffusion WebUI is supported on Intel CPUs, Intel integrated GPUs, and Intel discrete GPUs by leveraging OpenVINO torch.compile capability. Detailed instructions are available in[ Stable Diffusion WebUI repository](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/stable-diffusion-torchdynamo-backend/stable-diffusion-torchdynamo-backend.png?raw=true",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [
     "Stable Diffusion"
    ],
    "tasks": [
     "Text-to-Image"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
